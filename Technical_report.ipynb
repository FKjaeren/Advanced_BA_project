{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gower\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction import _stop_words\n",
    "import string\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Technical Report\n",
    "## Data preparation\n",
    "Data and preprocessing:\n",
    "In this project we will use two datasets. The first dataset contains ratings for groceries and gourmet foods and the second contains relevant information of the products. Both datasets are loaded with a function we call \"CreateData\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "### Import raw data\n",
    "def load_data(rating_filepath, review_filepath, metadata_filepath):\n",
    "    ratings_df = pd.read_csv(rating_filepath, names = ['item','user','rating','timestamp'])\n",
    "    reviews_df = pd.read_json(review_filepath, lines=True)\n",
    "    metadata_df = pd.read_json(metadata_filepath, lines=True)\n",
    "    return ratings_df, reviews_df, metadata_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then a preparation of the data is performed with the function \"prepare_data\". The purpose of this function is to merge the datasets and to structure the data. In the function we group on each item and we find the average rating, standard deviation of rating and the number of ratings for each item. \n",
    "The \"category\" feature is changed so it only contains the first category of a list instead of a list with multiple categories.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "### Function for preparing the data\n",
    "def prepare_data(ratings_df, reviews_df, metadata_df):\n",
    "    # create timestamps\n",
    "    ratings_df['timestamp'] = pd.to_datetime(ratings_df['timestamp'], origin = 'unix', unit = 's')\n",
    "    reviews_df['timestamp'] = pd.to_datetime(reviews_df['unixReviewTime'], origin = 'unix', unit = 's')\n",
    "    metadata_df['timestamp'] = pd.to_datetime(metadata_df['date'].apply(str), format = '%B %d, %Y', errors='coerce')\n",
    "\n",
    "    # drop columns in reviews\n",
    "    reviews_df = reviews_df.drop(columns=['unixReviewTime','reviewTime','reviewerName','vote','image','style','verified'])\n",
    "\n",
    "    # drop columns in metadata\n",
    "    metadata_df = metadata_df.drop(columns=['imageURL','imageURLHighRes'])\n",
    "    \n",
    "    # drop na's and duplicates\n",
    "    reviews_df = reviews_df.dropna()\n",
    "    reviews_df = reviews_df.drop_duplicates(keep='first')\n",
    "    ratings_df = ratings_df.drop_duplicates(keep='first')\n",
    "\n",
    "    # group ratings_df and merge with metadata, so there is one dataframe with both ratings and information of products\n",
    "    grouped_ratings = ratings_df[['item','rating']].groupby(by='item').agg({'rating':['mean','std'],'item':'size'}).rename(columns={'statistics':'avg_rating','item':'num_ratings'}).reset_index()\n",
    "    grouped_ratings.columns = ['_'.join(col).strip() if col[1] else col[0] for col in grouped_ratings.columns.values]\n",
    "    grouped_ratings = grouped_ratings.rename(columns = {'rating_mean':'avg_rating','rating_std':'std_rating','num_ratings_size':'num_ratings'})\n",
    "    metadata_df = grouped_ratings.merge(metadata_df, how='outer', left_on='item', right_on='asin')\n",
    "    metadata_df['item'].fillna(metadata_df['asin'], inplace=True)\n",
    "    metadata_df = metadata_df.drop(columns=['asin','date','tech1','tech2','fit'])\n",
    "\n",
    "    # preprocess price\n",
    "    metadata_df['price'] =  pd.to_numeric(metadata_df['price'].str.replace('$',''), errors='coerce')\n",
    "\n",
    "    # Fill nan with empty space and use the get_category function\n",
    "    metadata_df['category'] = metadata_df['category'].fillna('')\n",
    "    metadata_df['category'] = metadata_df['category'].apply(get_category)\n",
    "    \n",
    "\n",
    "    return reviews_df, metadata_df\n",
    "\n",
    "# Function to return only the first name in each category variable.\n",
    "def get_category(row):\n",
    "    if len(row) > 1:\n",
    "        category = row[1]\n",
    "    else:\n",
    "        category = row\n",
    "    return category\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the data is merged we save it as a new csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "rating_filepath = 'raw_data/Grocery_and_Gourmet_Food.csv'\n",
    "review_filepath = 'raw_data/Grocery_and_Gourmet_Food_5.json' \n",
    "metadata_filepath = 'raw_data/meta_Grocery_and_Gourmet_Food.json'\n",
    "\n",
    "raw_ratings, raw_reviews, raw_metadata = load_data(rating_filepath=rating_filepath, review_filepath=review_filepath, metadata_filepath=metadata_filepath)\n",
    "\n",
    "reviews_df, metadata_df = prepare_data(raw_ratings, raw_reviews, raw_metadata)\n",
    "\n",
    "# Save the new dataframes to later use. \n",
    "reviews_df.to_csv('data/reviews_df.csv',index=False)\n",
    "metadata_df.to_csv('data/metadata_df.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration\n",
    "The idea behind our investigation is to find features of high rated products and recommend to add the features to low rated products. However, if the products are too different like chocolate and apples it does not make sense to compare features for these products even if one has a high rating and the other a low rating. Therefore, we analyze the data by looking at the different product categories and explore the number of products, the number of ratings and variance in the average rating of the products. IT is important for our analysis that we have categories with both many products and many ratings. However it is also important that we have something to improve, meaning we need categories with some variances in the average rating of products. We will only be looking at the top 20 categories in the exploration. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# lead data \n",
    "df = pd.read_csv('data/metadata_df.csv')\n",
    "\n",
    "# Number of products in each category\n",
    "def get_category(row, categories):\n",
    "    if row in categories:\n",
    "        return row\n",
    "    else:\n",
    "        return ''\n",
    "\n",
    "# We will only look at the top 20 categories\n",
    "top = 20\n",
    "categories = df['category'].value_counts().sort_values(ascending=False).index[0:top].to_list()\n",
    "\n",
    "# Copy the dataframe\n",
    "df_category = df.copy(deep=True)\n",
    "\n",
    "# Return number of products in each category\n",
    "df_category['category'] = df_category['category'].apply(lambda row: get_category(row, categories))\n",
    "df_category = df_category[df_category['category'] != '']\n",
    "\n",
    "# Number of ratings in each category\n",
    "df_num_ratings = df[['category','num_ratings']].groupby(by=[\"category\"]).sum([\"num_ratings\"])\n",
    "df_num_ratings = df_num_ratings['num_ratings'].sort_values(ascending=False).reset_index()\n",
    "\n",
    "# Plot of the number of productts in each category and the number of ratings in each category\n",
    "fig, ((ax1, ax2)) = plt.subplots(1,2)\n",
    "sns.countplot(x=\"category\", data=df_category, order=categories, ax=ax1)\n",
    "ax1.set_ylabel('Number of products')\n",
    "ax1.set_xticklabels(categories,rotation=90)\n",
    "sns.barplot(x=\"category\", y=\"num_ratings\", data=df_num_ratings[0:top], ax=ax2)\n",
    "ax2.set_ylabel('Number of ratings')\n",
    "ax2.set_xticklabels(df_num_ratings.loc[0:(top-1),'category'].to_list(),rotation=90)\n",
    "fig.tight_layout()\n",
    "fig.show()\n",
    "\n",
    "# Plot with variance of average ratings in each category \n",
    "categories_union = list(set().union(categories,df_num_ratings.loc[0:top,'category'])) # list of categories shown in figure 1 and 2\n",
    "df_mean_avg_rating = df[df['category'].isin(categories_union)].groupby('category').median(['avg_rating']).sort_values(by='avg_rating',ascending=False)\n",
    "categories_union = df_mean_avg_rating.index.to_list()\n",
    "plt.figure(2)\n",
    "sns.boxplot(x = 'category', y = 'avg_rating', data = df[df['category'].isin(categories_union)], order = categories_union)\n",
    "plt.xticks(rotation=90)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By investigating the plot with number of ratings and number of products we find \"Beverages\", \"Cooking & Baking\", \"Candy & Chocolate\" and \"Snack Foods\" are the categories with most in both. When we compare these categories with the boxplot showing the variance of the average rating, both \"Snack Foods\" and \"Candy & Chocolate\" have a decent spread. \"Beverages\" and \"Cooking & Baking\" almost have the same median and spread, but \"Beverages\" has both more products and ratings and we therefore only use \"Beverages\" as a category along with \"Candy & Chocolate\" and \"Snack Foods\". \n",
    "\n",
    "These three categories are then saved as csv file separetily. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Select category \n",
    "category = 'Snack Foods'\n",
    "df_cat = df[df['category']==category]\n",
    "\n",
    "# Save dataframe for category \n",
    "df_cat.to_csv('data/'+category+'/df_'+category+'.csv',index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing\n",
    "So far we have found 3 categories we want to use for our hypothesis. These 3 datasets needs to further processed before we can use them for modelling. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Function to preprocess the train and test dataset\n",
    "def preprocess_data(df_train, df_test):\n",
    "    # get number of also_buy\n",
    "    df_train['also_buy'] = df_train['also_buy'].fillna('').apply(get_number_also_buy)\n",
    "    df_test['also_buy'] = df_test['also_buy'].fillna('').apply(get_number_also_buy)\n",
    "\n",
    "    # sales rank information\n",
    "    df_train['rank'] = df_train['rank'].apply(get_rank).str.replace(',','').str.extract('(\\d+|$)')\n",
    "    df_train['rank'] = pd.to_numeric(df_train['rank'], errors = 'coerce').fillna(0).apply(int)\n",
    "    df_test['rank'] = df_test['rank'].apply(get_rank).str.replace(',','').str.extract('(\\d+|$)')\n",
    "    df_test['rank'] = pd.to_numeric(df_test['rank'], errors = 'coerce').fillna(0).apply(int)\n",
    "\n",
    "    # Fill nan values for price data\n",
    "    # get number of also_view\n",
    "    df_train['also_view'] = df_train['also_view'].fillna('').apply(get_number_also_buy)\n",
    "    df_test['also_view'] = df_test['also_view'].fillna('').apply(get_number_also_buy)\n",
    "\n",
    "    # Clean description\n",
    "    df_train['description'] = df_train['description'].apply(get_description)\n",
    "    # Drop rows with no information\n",
    "    df_train = df_train.dropna(axis = 0, subset=['description'])\n",
    "    # Make it a string and clean html text\n",
    "    df_train['description'] = df_train['description'].apply(str)\n",
    "    df_train['description'] = df_train['description'].str.replace('\\n', '')\n",
    "    df_train['description'] = df_train[['description']].applymap(lambda text: BeautifulSoup(text, 'html.parser').get_text())\n",
    "    # Perform text processing where stop words are removed etc. \n",
    "    df_train['description'] = df_train['description'].apply(text_processing)\n",
    "\n",
    "    # Do the same for test dataset\n",
    "    df_test['description'] = df_test['description'].apply(get_description)\n",
    "    df_test = df_test.dropna(axis = 0, subset=['description'])\n",
    "    df_test['description'] = df_test['description'].apply(str)\n",
    "    df_test['description'] = df_test['description'].str.replace('\\n', '')\n",
    "    df_test['description'] = df_test[['description']].applymap(lambda text: BeautifulSoup(text, 'html.parser').get_text())\n",
    "    df_test['description'] = df_test['description'].apply(text_processing)\n",
    "    return df_train, df_test\n",
    "\n",
    "# Number of also bougtht products\n",
    "def get_number_also_buy(row):\n",
    "    number = len(row)\n",
    "    return number\n",
    "# Get the brand\n",
    "def get_brand(row, brands):\n",
    "    if row in brands:\n",
    "        return row\n",
    "    else:\n",
    "        return 'Other'\n",
    "# Get the rank from list\n",
    "def get_rank(row):\n",
    "    if isinstance(row, list):\n",
    "        if len(row) > 0:\n",
    "            return row[0]\n",
    "        else:\n",
    "            return ''\n",
    "    else:\n",
    "        return row\n",
    "\n",
    "# Use only rows with list of information else nan\n",
    "def get_description(row):\n",
    "    if isinstance(row, list):\n",
    "        if len(row)>0:\n",
    "            return row\n",
    "        else:\n",
    "            return np.nan\n",
    "    else:\n",
    "        return row\n",
    "\n",
    "# Function used to clean text data\n",
    "def text_processing(text):\n",
    "    # remove punctuation \n",
    "    text = \"\".join([c for c in text \n",
    "        if c not in string.punctuation])\n",
    "    # lowercase\n",
    "    text = \"\".join([c.lower() for c in text])\n",
    "    # stemming / lematizing (optional)\n",
    "    text = \" \".join([lemmatizer.lemmatize(w) for w in text.split()])\n",
    "    # remove stopwords\n",
    "    text = \" \".join([w for w in text.split() \n",
    "        if w not in Stop_Words])\n",
    "    return text\n",
    "# Function for preprocessing the prize where the data also will splitted in a train and a test set. \n",
    "def preprocess_price(metadata_df):\n",
    "    # Drop columns we don't use\n",
    "    df = metadata_df.drop(columns = ['item','title','feature','main_cat','similar_item','details','timestamp'])\n",
    "    # Empty columns can't be used for anything\n",
    "    df = df.dropna(axis=0,subset=['avg_rating','num_ratings','description'])\n",
    "    # Split the data in a 75 % split train and test set. \n",
    "    df_train, df_test = train_test_split(df, train_size=0.75)\n",
    "\n",
    "    # Hvordan skal det her forstås?\n",
    "    categories = []\n",
    "    category_means = []\n",
    "    categories = df_train.category.unique()\n",
    "    for i in categories:\n",
    "        temp = df_train[df_train['price'].isna() == False]\n",
    "        mean_value = temp[temp['category'] == i]['price'].mean()\n",
    "        category_means.append(mean_value)\n",
    "    dict = {'categories': categories,'category_means': category_means}\n",
    "    category_stat_df = pd.DataFrame(dict)\n",
    "    category_stat_df = category_stat_df.set_index('categories')\n",
    "\n",
    "    df_train['price'] = df_train.apply(lambda row: category_stat_df.loc[row['category']].values[0] if row['price'] != row['price'] else row['price'], axis = 1)\n",
    "    df_test['price'] = df_test.apply(lambda row: category_stat_df.loc[row['category']].values[0] if row['price'] != row['price'] else row['price'], axis = 1)\n",
    "    columns = df_train.columns\n",
    "    if 'category' in columns:\n",
    "        df_train = df_train.drop(columns = ['category'])\n",
    "        df_test = df_test.drop(columns = ['category'])\n",
    "    if 'orig category' in columns:\n",
    "        df_train = df_train.drop(columns = ['orig category'])\n",
    "        df_test = df_test.drop(columns = ['orig category'])\n",
    "    return df_train, df_test\n",
    "\n",
    "\n",
    "\n",
    "# Words from already trained lda model. The words are removed because they either occur in multiple topics or doesn't make sense in the topic. \n",
    "category = 'Snack Foods'\n",
    "if category == 'Candy & Chocolate':\n",
    "    Stop_Words = _stop_words.ENGLISH_STOP_WORDS.union(['chocolate','supplement','cocoa','candy','cure','condition'])\n",
    "elif category == 'Snack Foods':\n",
    "    Stop_Words = _stop_words.ENGLISH_STOP_WORDS.union(['snack','food','fda','flavor','product','ingredient','statement'])\n",
    "elif category == 'Beverages':\n",
    "    Stop_Words = _stop_words.ENGLISH_STOP_WORDS.union(['tea','coffee','water','cup','supplement','flavor','year','food','condition'])\n",
    "\n",
    "# Kjær tror det er bedst du skriver kommentarer her\n",
    "metadata_df = pd.read_csv('data/'+category+'/df_'+category+'.csv')\n",
    "metadata_df['orig category'] = metadata_df['category']\n",
    "dummy_df = pd.get_dummies(metadata_df, columns=['brand','orig category'])\n",
    "metadata_df = metadata_df.drop(columns=['brand'])\n",
    "\n",
    "df_train, df_test = preprocess_price(metadata_df)\n",
    "df_train_dummy, df_test_dummy = preprocess_price(dummy_df)\n",
    "\n",
    "df_train, df_test = preprocess_data(df_train, df_test)\n",
    "df_train_dummy, df_test_dummy = preprocess_data(df_train_dummy, df_test_dummy)\n",
    "\n",
    "df_train_dummy = df_train_dummy.drop(columns = ['description','std_rating'])\n",
    "df_test_dummy = df_test_dummy.drop(columns = ['description','std_rating'])\n",
    "\n",
    "pca = PCA(n_components=100)\n",
    "pca.fit(df_train_dummy)\n",
    "print(pca.explained_variance_ratio_)\n",
    "pca = PCA(n_components=2).fit(df_train_dummy)\n",
    "pca_values = pca.fit_transform(df_train_dummy)\n",
    "kmeans = KMeans(n_clusters=5).fit(pca_values)\n",
    "df_train['cluster'] = kmeans.labels_\n",
    "pca_values_test = pca.transform(df_test_dummy)\n",
    "df_test['cluster'] = kmeans.predict(pca_values_test)\n",
    "\n",
    "df_train = pd.get_dummies(df_train, columns = ['cluster'])\n",
    "df_test = pd.get_dummies(df_test, columns = ['cluster'])\n",
    "\n",
    "# ensure same shape of train and test\n",
    "if df_train.shape[1] != df_test.shape[1]:\n",
    "    setdiff = set(df_train.columns).difference(set(df_test.columns))\n",
    "    for name in setdiff:\n",
    "        df_test[name] = np.zeros(df_test.shape[0])\n",
    "        df_test = df_test.astype({name:'int'})\n",
    "        \n",
    "# order columns in test set\n",
    "df_test = df_test[df_train.columns]     \n",
    "\n",
    "#sns.scatterplot('price','num_ratings', hue = 'cluster', data = df_train)\n",
    "#plt.show()\n",
    "\n",
    "df_train.to_csv('data/' + category + '/df_train.csv',index=False)\n",
    "df_test.to_csv('data/' + category + '/df_test.csv',index=False)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
