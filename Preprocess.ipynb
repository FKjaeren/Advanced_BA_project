{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/carolinesofieljorring/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/carolinesofieljorring/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/carolinesofieljorring/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "## Packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "# For text processing \n",
    "import string\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.feature_extraction import _stop_words\n",
    "Stop_Words= _stop_words.ENGLISH_STOP_WORDS\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# For sentiment analysis \n",
    "nltk.download('vader_lexicon')\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore') # ignore warnings\n",
    "\n",
    "np.random.seed(42) # set seed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check category:  ['Candy & Chocolate']\n"
     ]
    }
   ],
   "source": [
    "# load data \n",
    "category = 'Candy & Chocolate' # define category \n",
    "metadata_df = pd.read_csv('data/'+category+'/df_'+category+'.csv')\n",
    "print('Check category: ', metadata_df.category.unique())\n",
    "# drop features not used for modeling\n",
    "metadata_df = metadata_df.drop(columns = ['item','std_rating','category','brand','feature','main_cat','similar_item','details','timestamp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "0         1.0\n1        48.0\n2         2.0\n3         1.0\n4         1.0\n         ... \n40663     1.0\n40664     2.0\n40665     1.0\n40666     1.0\n40667     1.0\nName: num_ratings, Length: 40668, dtype: float64"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata_df.num_ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Functions for preprocessing of features\n",
    "def get_number_also_buy(row):\n",
    "    number = len(row)\n",
    "    return number\n",
    "\n",
    "def get_brand(row, brands):\n",
    "    if row in brands:\n",
    "        return row\n",
    "    else:\n",
    "        return 'Other'\n",
    "\n",
    "def get_rank(row):\n",
    "    if isinstance(row, list):\n",
    "        if len(row) > 0:\n",
    "            return row[0]\n",
    "        else:\n",
    "            return ''\n",
    "    else:\n",
    "        return row\n",
    "\n",
    "def get_description(row):\n",
    "    if isinstance(row, list):\n",
    "        if len(row)>0:\n",
    "            return row\n",
    "        else:\n",
    "            return np.nan\n",
    "    else:\n",
    "        return row\n",
    "\n",
    "def get_length(row):\n",
    "    if isinstance(row, list):\n",
    "        if len(row)>0:\n",
    "            return len(row)\n",
    "        else:\n",
    "            return np.nan\n",
    "    else:\n",
    "        return len(row)\n",
    "\n",
    "def text_processing(text):\n",
    "    # remove punctuation \n",
    "    text = \"\".join([c for c in text \n",
    "        if c not in string.punctuation])\n",
    "    # lowercase\n",
    "    text = \"\".join([c.lower() for c in text])\n",
    "    # remove stopwords\n",
    "    text = \" \".join([w for w in text.split() \n",
    "        if w not in Stop_Words])\n",
    "    # stemming / lematizing (optional)\n",
    "    text = \" \".join([lemmatizer.lemmatize(w) for w in text.split()])\n",
    "    return text\n",
    "\n",
    "# sentimental analysis of description using pretrained sentiment model \n",
    "def get_sentiment(row):\n",
    "        compound = sid.polarity_scores(row)['compound']\n",
    "        return compound "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of products with missing price:  15575\n",
      "number of rows with price set to average price:  15575\n"
     ]
    }
   ],
   "source": [
    "## Preprocess data\n",
    "def preprocess_data(metadata_df):   \n",
    "    df_train, df_test = train_test_split(metadata_df, train_size=0.75) # split data, so we DON'T use test for preprocessing\n",
    "\n",
    "    # get number of also_buy\n",
    "    df_train['also_buy'] = df_train['also_buy'].fillna('').apply(get_number_also_buy)\n",
    "    df_test['also_buy'] = df_test['also_buy'].fillna('').apply(get_number_also_buy)\n",
    "\n",
    "    # get number of also_view\n",
    "    df_train['also_view'] = df_train['also_view'].fillna('').apply(get_number_also_buy)\n",
    "    df_test['also_view'] = df_test['also_view'].fillna('').apply(get_number_also_buy)\n",
    "\n",
    "    # sales rank information\n",
    "    df_train['rank'] = df_train['rank'].apply(get_rank).str.replace(',','').str.extract('(\\d+|$)')\n",
    "    df_train['rank'] = pd.to_numeric(df_train['rank'], errors = 'coerce').fillna(0).apply(int)\n",
    "    df_test['rank'] = df_test['rank'].apply(get_rank).str.replace(',','').str.extract('(\\d+|$)')\n",
    "    df_test['rank'] = pd.to_numeric(df_test['rank'], errors = 'coerce').fillna(0).apply(int)\n",
    "    # remove samples where rank = 0 (not assigned)\n",
    "    df_train = df_train[df_train['rank']>0]\n",
    "    df_test = df_test[df_test['rank']>0]\n",
    "\n",
    "    # get title length\n",
    "    df_train['title_length'] = df_train['title'].apply(get_length)\n",
    "    df_test['title_length'] = df_test['title'].apply(get_length)\n",
    "\n",
    "    # get description length\n",
    "    df_train['description_length'] = df_train['description'].apply(get_length)\n",
    "    df_test['description_length'] = df_test['description'].apply(get_length)\n",
    "\n",
    "    # clean description\n",
    "    df_train['description'] = df_train['description'].apply(get_description)\n",
    "    df_train = df_train.dropna(axis = 0, subset=['description'])\n",
    "    df_train['description'] = df_train['description'].apply(str)\n",
    "    df_train['description'] = df_train['description'].str.replace('\\n', '')\n",
    "    df_train['description'] = df_train[['description']].applymap(lambda text: BeautifulSoup(text, 'html.parser').get_text())\n",
    "    df_train['description'] = df_train['description'].apply(text_processing)\n",
    "    df_test['description'] = df_test['description'].apply(get_description)\n",
    "    df_test = df_test.dropna(axis = 0, subset=['description'])\n",
    "    df_test['description'] = df_test['description'].apply(str)\n",
    "    df_test['description'] = df_test['description'].str.replace('\\n', '')\n",
    "    df_test['description'] = df_test[['description']].applymap(lambda text: BeautifulSoup(text, 'html.parser').get_text())\n",
    "    df_test['description'] = df_test['description'].apply(text_processing)\n",
    "    \n",
    "    # get sentiment score of description \n",
    "    df_train['description_sentiment'] = df_train['description'].apply(get_sentiment) # compound score\n",
    "    df_test['description_sentiment'] = df_test['description'].apply(get_sentiment) \n",
    "    \n",
    "    # set price=nan to average price \n",
    "    temp = df_train[df_train['price'].isna() == False]\n",
    "    print('number of products with missing price: ', (df_train.shape[0]-temp.shape[0]))\n",
    "    mean_value = temp['price'].mean()\n",
    "    df_train['price'] = df_train.apply(lambda row: mean_value if row['price'] != row['price'] else row['price'], axis = 1)\n",
    "    df_test['price'] = df_test.apply(lambda row: mean_value if row['price'] != row['price'] else row['price'], axis = 1)\n",
    "    print('number of rows with price set to average price: ', (df_train.price == mean_value).sum())\n",
    "\n",
    "    # scale \n",
    "    features_to_scale = ['avg_rating','num_ratings','rank','also_buy','also_view','price','title_length','description_length']\n",
    "    scaler = MinMaxScaler()\n",
    "    df_train[features_to_scale] = scaler.fit_transform(df_train[features_to_scale])\n",
    "    df_test[features_to_scale] = scaler.transform(df_test[features_to_scale])\n",
    "\n",
    "    return df_train, df_test\n",
    "\n",
    "df_train, df_test = preprocess_data(metadata_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.to_csv('data/' + category + '/df_train.csv',index=False)\n",
    "df_test.to_csv('data/' + category + '/df_test.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 64-bit ('BA': conda)",
   "name": "python3912jvsc74a57bd02d45b184501721afdaf308c0e19a11b5a6bc7506660baeaf0fca16878e4c77d3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "metadata": {
   "interpreter": {
    "hash": "2d45b184501721afdaf308c0e19a11b5a6bc7506660baeaf0fca16878e4c77d3"
   }
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}