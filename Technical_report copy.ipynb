{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gower\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction import _stop_words\n",
    "import string\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Technical Report\n",
    "## Data preparation\n",
    "Data and preprocessing:\n",
    "In this project we will use two datasets. The first dataset contains ratings for groceries and gourmet foods and the second contains relevant information of the products. Both datasets are loaded with a function we call \"CreateData\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Import raw data\n",
    "def load_data(rating_filepath, review_filepath, metadata_filepath):\n",
    "    ratings_df = pd.read_csv(rating_filepath, names = ['item','user','rating','timestamp'])\n",
    "    reviews_df = pd.read_json(review_filepath, lines=True)\n",
    "    metadata_df = pd.read_json(metadata_filepath, lines=True)\n",
    "    return ratings_df, reviews_df, metadata_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then a preparation of the data is performed with the function \"prepare_data\". The purpose of this function is to merge the datasets and to structure the data. In the function we group on each item and we find the average rating, standard deviation of rating and the number of ratings for each item. \n",
    "The \"category\" feature is changed so it only contains the first category of a list instead of a list with multiple categories.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Function for preparing the data\n",
    "def prepare_data(ratings_df, reviews_df, metadata_df):\n",
    "    # create timestamps\n",
    "    ratings_df['timestamp'] = pd.to_datetime(ratings_df['timestamp'], origin = 'unix', unit = 's')\n",
    "    reviews_df['timestamp'] = pd.to_datetime(reviews_df['unixReviewTime'], origin = 'unix', unit = 's')\n",
    "    metadata_df['timestamp'] = pd.to_datetime(metadata_df['date'].apply(str), format = '%B %d, %Y', errors='coerce')\n",
    "\n",
    "    # drop columns in reviews\n",
    "    reviews_df = reviews_df.drop(columns=['unixReviewTime','reviewTime','reviewerName','vote','image','style','verified'])\n",
    "\n",
    "    # drop columns in metadata\n",
    "    metadata_df = metadata_df.drop(columns=['imageURL','imageURLHighRes'])\n",
    "    \n",
    "    # drop na's and duplicates\n",
    "    reviews_df = reviews_df.dropna()\n",
    "    reviews_df = reviews_df.drop_duplicates(keep='first')\n",
    "    ratings_df = ratings_df.drop_duplicates(keep='first')\n",
    "\n",
    "    # group ratings_df and merge with metadata, so there is one dataframe with both ratings and information of products\n",
    "    grouped_ratings = ratings_df[['item','rating']].groupby(by='item').agg({'rating':['mean','std'],'item':'size'}).rename(columns={'statistics':'avg_rating','item':'num_ratings'}).reset_index()\n",
    "    grouped_ratings.columns = ['_'.join(col).strip() if col[1] else col[0] for col in grouped_ratings.columns.values]\n",
    "    grouped_ratings = grouped_ratings.rename(columns = {'rating_mean':'avg_rating','rating_std':'std_rating','num_ratings_size':'num_ratings'})\n",
    "    metadata_df = grouped_ratings.merge(metadata_df, how='outer', left_on='item', right_on='asin')\n",
    "    metadata_df['item'].fillna(metadata_df['asin'], inplace=True)\n",
    "    metadata_df = metadata_df.drop(columns=['asin','date','tech1','tech2','fit'])\n",
    "\n",
    "    # preprocess price\n",
    "    metadata_df['price'] =  pd.to_numeric(metadata_df['price'].str.replace('$',''), errors='coerce')\n",
    "\n",
    "    # Fill nan with empty space and use the get_category function\n",
    "    metadata_df['category'] = metadata_df['category'].fillna('')\n",
    "    metadata_df['category'] = metadata_df['category'].apply(get_category)\n",
    "    \n",
    "\n",
    "    return reviews_df, metadata_df\n",
    "\n",
    "# Function to return only the first name in each category variable.\n",
    "def get_category(row):\n",
    "    if len(row) > 1:\n",
    "        category = row[1]\n",
    "    else:\n",
    "        category = row\n",
    "    return category\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the data is merged we save it as a new csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_filepath = 'raw_data/Grocery_and_Gourmet_Food.csv'\n",
    "review_filepath = 'raw_data/Grocery_and_Gourmet_Food_5.json' \n",
    "metadata_filepath = 'raw_data/meta_Grocery_and_Gourmet_Food.json'\n",
    "\n",
    "raw_ratings, raw_reviews, raw_metadata = load_data(rating_filepath=rating_filepath, review_filepath=review_filepath, metadata_filepath=metadata_filepath)\n",
    "\n",
    "reviews_df, metadata_df = prepare_data(raw_ratings, raw_reviews, raw_metadata)\n",
    "\n",
    "# Save the new dataframes to later use. \n",
    "reviews_df.to_csv('data/reviews_df.csv',index=False)\n",
    "metadata_df.to_csv('data/metadata_df.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration\n",
    "The idea behind our investigation is to find features of high rated products and recommend to add the features to low rated products. However, if the products are too different like chocolate and apples it does not make sense to compare features for these products even if one has a high rating and the other a low rating. Therefore, we analyze the data by looking at the different product categories and explore the number of products, the number of ratings and variance in the average rating of the products. IT is important for our analysis that we have categories with both many products and many ratings. However it is also important that we have something to improve, meaning we need categories with some variances in the average rating of products. We will only be looking at the top 20 categories in the exploration. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lead data \n",
    "df = pd.read_csv('data/metadata_df.csv')\n",
    "\n",
    "# Number of products in each category\n",
    "def get_category(row, categories):\n",
    "    if row in categories:\n",
    "        return row\n",
    "    else:\n",
    "        return ''\n",
    "\n",
    "# We will only look at the top 20 categories\n",
    "top = 20\n",
    "categories = df['category'].value_counts().sort_values(ascending=False).index[0:top].to_list()\n",
    "\n",
    "# Copy the dataframe\n",
    "df_category = df.copy(deep=True)\n",
    "\n",
    "# Return number of products in each category\n",
    "df_category['category'] = df_category['category'].apply(lambda row: get_category(row, categories))\n",
    "df_category = df_category[df_category['category'] != '']\n",
    "\n",
    "# Number of ratings in each category\n",
    "df_num_ratings = df[['category','num_ratings']].groupby(by=[\"category\"]).sum([\"num_ratings\"])\n",
    "df_num_ratings = df_num_ratings['num_ratings'].sort_values(ascending=False).reset_index()\n",
    "\n",
    "# Plot of the number of productts in each category and the number of ratings in each category\n",
    "fig, ((ax1, ax2)) = plt.subplots(1,2)\n",
    "sns.countplot(x=\"category\", data=df_category, order=categories, ax=ax1)\n",
    "ax1.set_ylabel('Number of products')\n",
    "ax1.set_xticklabels(categories,rotation=90)\n",
    "sns.barplot(x=\"category\", y=\"num_ratings\", data=df_num_ratings[0:top], ax=ax2)\n",
    "ax2.set_ylabel('Number of ratings')\n",
    "ax2.set_xticklabels(df_num_ratings.loc[0:(top-1),'category'].to_list(),rotation=90)\n",
    "fig.tight_layout()\n",
    "fig.show()\n",
    "\n",
    "# Plot with variance of average ratings in each category \n",
    "categories_union = list(set().union(categories,df_num_ratings.loc[0:top,'category'])) # list of categories shown in figure 1 and 2\n",
    "df_mean_avg_rating = df[df['category'].isin(categories_union)].groupby('category').median(['avg_rating']).sort_values(by='avg_rating',ascending=False)\n",
    "categories_union = df_mean_avg_rating.index.to_list()\n",
    "plt.figure(2)\n",
    "sns.boxplot(x = 'category', y = 'avg_rating', data = df[df['category'].isin(categories_union)], order = categories_union)\n",
    "plt.xticks(rotation=90)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By investigating the plot with number of ratings and number of products we find \"Beverages\", \"Cooking & Baking\", \"Candy & Chocolate\" and \"Snack Foods\" are the categories with most in both. When we compare these categories with the boxplot showing the variance of the average rating, both \"Snack Foods\" and \"Candy & Chocolate\" have a decent spread. \"Beverages\" and \"Cooking & Baking\" almost have the same median and spread, but \"Beverages\" has both more products and ratings and we therefore only use \"Beverages\" as a category along with \"Candy & Chocolate\" and \"Snack Foods\". \n",
    "\n",
    "These three categories are then saved as csv file separetily. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select category \n",
    "category = 'Snack Foods'\n",
    "df_cat = df[df['category']==category]\n",
    "\n",
    "# Save dataframe for category \n",
    "df_cat.to_csv('data/'+category+'/df_'+category+'.csv',index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing\n",
    "So far we have found 3 categories we want to use for our hypothesis and saved them as separate dataframes. These 3 datasets needs to be further processed before we can use them for modelling. In the function \"preprocess_price\" we are splitting our datasets into a training set and a test set. We are also finding the mean price for each category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for preprocessing the prize where the data also will splitted in a train and a test set. \n",
    "def preprocess_price(metadata_df):\n",
    "    # Drop columns we don't use\n",
    "    df = metadata_df.drop(columns = ['item','title','feature','main_cat','similar_item','details','timestamp'])\n",
    "    # Empty columns can't be used for anything\n",
    "    df = df.dropna(axis=0,subset=['avg_rating','num_ratings','description'])\n",
    "    # Split the data in a 75 % split train and test set. \n",
    "    df_train, df_test = train_test_split(df, train_size=0.75)\n",
    "\n",
    "    # Below we find the mean price for every cateogry. \n",
    "    categories = []\n",
    "    category_means = []\n",
    "    categories = df_train.category.unique()\n",
    "    for i in categories:\n",
    "        temp = df_train[df_train['price'].isna() == False]\n",
    "        mean_value = temp[temp['category'] == i]['price'].mean()\n",
    "        category_means.append(mean_value)\n",
    "    dict = {'categories': categories,'category_means': category_means}\n",
    "    category_stat_df = pd.DataFrame(dict)\n",
    "    category_stat_df = category_stat_df.set_index('categories')\n",
    "\n",
    "    # Next for all NULL values we assign the price to be the mean price in that category.\n",
    "\n",
    "    df_train['price'] = df_train.apply(lambda row: category_stat_df.loc[row['category']].values[0] if row['price'] != row['price'] else row['price'], axis = 1)\n",
    "    df_test['price'] = df_test.apply(lambda row: category_stat_df.loc[row['category']].values[0] if row['price'] != row['price'] else row['price'], axis = 1)\n",
    "    \n",
    "    # Here we drop the category column. \n",
    "    columns = df_train.columns\n",
    "    if 'category' in columns:\n",
    "        df_train = df_train.drop(columns = ['category'])\n",
    "        df_test = df_test.drop(columns = ['category'])\n",
    "    if 'orig category' in columns:\n",
    "        df_train = df_train.drop(columns = ['orig category'])\n",
    "        df_test = df_test.drop(columns = ['orig category'])\n",
    "    return df_train, df_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training set and the test set are now further processed in the \"preprocess_data\" function. In this function we clean the numeric features and the text features. In the numeric features  are removed from the numeric features and  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to preprocess the train and test dataset\n",
    "def preprocess_data(df_train, df_test):\n",
    "    # get number of also_buy\n",
    "    df_train['also_buy'] = df_train['also_buy'].fillna('').apply(get_number_also_buy)\n",
    "    df_test['also_buy'] = df_test['also_buy'].fillna('').apply(get_number_also_buy)\n",
    "\n",
    "    # sales rank information\n",
    "    df_train['rank'] = df_train['rank'].apply(get_rank).str.replace(',','').str.extract('(\\d+|$)')\n",
    "    df_train['rank'] = pd.to_numeric(df_train['rank'], errors = 'coerce').fillna(0).apply(int)\n",
    "    df_test['rank'] = df_test['rank'].apply(get_rank).str.replace(',','').str.extract('(\\d+|$)')\n",
    "    df_test['rank'] = pd.to_numeric(df_test['rank'], errors = 'coerce').fillna(0).apply(int)\n",
    "\n",
    "    # Fill nan values for price data\n",
    "    # get number of also_view\n",
    "    df_train['also_view'] = df_train['also_view'].fillna('').apply(get_number_also_buy)\n",
    "    df_test['also_view'] = df_test['also_view'].fillna('').apply(get_number_also_buy)\n",
    "\n",
    "    # Clean description\n",
    "    df_train['description'] = df_train['description'].apply(get_description)\n",
    "    # Drop rows with no information\n",
    "    df_train = df_train.dropna(axis = 0, subset=['description'])\n",
    "    # Make it a string and clean html text\n",
    "    df_train['description'] = df_train['description'].apply(str)\n",
    "    df_train['description'] = df_train['description'].str.replace('\\n', '')\n",
    "    df_train['description'] = df_train[['description']].applymap(lambda text: BeautifulSoup(text, 'html.parser').get_text())\n",
    "    # Perform text processing where stop words are removed etc. \n",
    "    df_train['description'] = df_train['description'].apply(text_processing)\n",
    "\n",
    "    # Do the same for test dataset\n",
    "    df_test['description'] = df_test['description'].apply(get_description)\n",
    "    df_test = df_test.dropna(axis = 0, subset=['description'])\n",
    "    df_test['description'] = df_test['description'].apply(str)\n",
    "    df_test['description'] = df_test['description'].str.replace('\\n', '')\n",
    "    df_test['description'] = df_test[['description']].applymap(lambda text: BeautifulSoup(text, 'html.parser').get_text())\n",
    "    df_test['description'] = df_test['description'].apply(text_processing)\n",
    "    return df_train, df_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Number of also bougtht products\n",
    "def get_number_also_buy(row):\n",
    "    number = len(row)\n",
    "    return number\n",
    "# Get the brand\n",
    "def get_brand(row, brands):\n",
    "    if row in brands:\n",
    "        return row\n",
    "    else:\n",
    "        return 'Other'\n",
    "# Get the rank from list\n",
    "def get_rank(row):\n",
    "    if isinstance(row, list):\n",
    "        if len(row) > 0:\n",
    "            return row[0]\n",
    "        else:\n",
    "            return ''\n",
    "    else:\n",
    "        return row\n",
    "\n",
    "# Use only rows with list of information else nan\n",
    "def get_description(row):\n",
    "    if isinstance(row, list):\n",
    "        if len(row)>0:\n",
    "            return row\n",
    "        else:\n",
    "            return np.nan\n",
    "    else:\n",
    "        return row\n",
    "\n",
    "# Function used to clean text data\n",
    "def text_processing(text):\n",
    "    # remove punctuation \n",
    "    text = \"\".join([c for c in text \n",
    "        if c not in string.punctuation])\n",
    "    # lowercase\n",
    "    text = \"\".join([c.lower() for c in text])\n",
    "    # stemming / lematizing (optional)\n",
    "    text = \" \".join([lemmatizer.lemmatize(w) for w in text.split()])\n",
    "    # remove stopwords\n",
    "    text = \" \".join([w for w in text.split() \n",
    "        if w not in Stop_Words])\n",
    "    return text\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Words from already trained lda model. The words are removed because they either occur in multiple topics or doesn't make sense in the topic. \n",
    "category = 'Snack Foods'\n",
    "if category == 'Candy & Chocolate':\n",
    "    Stop_Words = _stop_words.ENGLISH_STOP_WORDS.union(['chocolate','supplement','cocoa','candy','cure','condition'])\n",
    "elif category == 'Snack Foods':\n",
    "    Stop_Words = _stop_words.ENGLISH_STOP_WORDS.union(['snack','food','fda','flavor','product','ingredient','statement'])\n",
    "elif category == 'Beverages':\n",
    "    Stop_Words = _stop_words.ENGLISH_STOP_WORDS.union(['tea','coffee','water','cup','supplement','flavor','year','food','condition'])\n",
    "\n",
    "# First we read the \"prepared\" data and the proper category found in the \"exploratory part\"\n",
    "metadata_df = pd.read_csv('data/'+category+'/df_'+category+'.csv')\n",
    "\n",
    "#We clone the category, as we will need this in order to preprocess price, but we will remove the original category column, \n",
    "# when we create dummy data\n",
    "metadata_df['orig category'] = metadata_df['category']\n",
    "dummy_df = pd.get_dummies(metadata_df, columns=['brand','orig category'])\n",
    "# We drop brand as we can't have non numerical values. \n",
    "metadata_df = metadata_df.drop(columns=['brand'])\n",
    "\n",
    "# We apply the preprocess price function, which handles some of the preprocess functions as well as splits the data in train and test.\n",
    "df_train, df_test = preprocess_price(metadata_df)\n",
    "df_train_dummy, df_test_dummy = preprocess_price(dummy_df)\n",
    "\n",
    "# Next the second part of the preprocess will be applied.\n",
    "df_train, df_test = preprocess_data(df_train, df_test)\n",
    "df_train_dummy, df_test_dummy = preprocess_data(df_train_dummy, df_test_dummy)\n",
    "\n",
    "## We drop description when we do pca on the data as it is non-numerical. Also we drop std_rating as it is quite hightly correlated with \n",
    "# Avg. rating, and it seems unlikely that std_rating can be gathered in a situation where average rating (the target) can't.\n",
    "df_train_dummy = df_train_dummy.drop(columns = ['description','std_rating'])\n",
    "df_test_dummy = df_test_dummy.drop(columns = ['description','std_rating'])\n",
    "\n",
    "## Next we decompose our data.\n",
    "pca = PCA(n_components=100)\n",
    "pca.fit(df_train_dummy)\n",
    "print(pca.explained_variance_ratio_)\n",
    "\n",
    "## The explained variance have been calculated, and it seems only 2 components are neccesary.\n",
    "pca = PCA(n_components=2).fit(df_train_dummy)\n",
    "pca_values = pca.fit_transform(df_train_dummy)\n",
    "\n",
    "## Then we create 5 clusters using KMeans.\n",
    "kmeans = KMeans(n_clusters=5).fit(pca_values)\n",
    "\n",
    "## We assign these clusters to our original (preprocessed) data.\n",
    "df_train['cluster'] = kmeans.labels_\n",
    "pca_values_test = pca.transform(df_test_dummy)\n",
    "df_test['cluster'] = kmeans.predict(pca_values_test)\n",
    "\n",
    "## We one hot encode the clusters, so that any model trained on the data, won't assume there is a relationship, between the values.\n",
    "df_train = pd.get_dummies(df_train, columns = ['cluster'])\n",
    "df_test = pd.get_dummies(df_test, columns = ['cluster'])\n",
    "\n",
    "# ensure same shape of train and test\n",
    "if df_train.shape[1] != df_test.shape[1]:\n",
    "    setdiff = set(df_train.columns).difference(set(df_test.columns))\n",
    "    for name in setdiff:\n",
    "        df_test[name] = np.zeros(df_test.shape[0])\n",
    "        df_test = df_test.astype({name:'int'})\n",
    "        \n",
    "# order columns in test set\n",
    "df_test = df_test[df_train.columns]     \n",
    "\n",
    "#sns.scatterplot('price','num_ratings', hue = 'cluster', data = df_train)\n",
    "#plt.show()\n",
    "\n",
    "df_train.to_csv('data/' + category + '/df_train.csv',index=False)\n",
    "df_test.to_csv('data/' + category + '/df_test.csv',index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA Modeling\n",
    "\n",
    "After the preprocessing of the feature \"description\", we will now make a last \"pre-process\" step in order to make every single feature numeric. What we will do is we will decompose the feature description using the Latent Dirichlet Allocation to find topics in the desciptions of our products. Doing so we hope to have found a type of cluster of words, we can use to create models later, which can predict rating and sales rank of our products. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "julia"
    }
   },
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from datetime import date\n",
    "import wordcloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# set random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "#%% Functions \n",
    "def get_len(text):\n",
    "    if text != text:\n",
    "        return 0\n",
    "    elif isinstance(text, float):\n",
    "        return 1\n",
    "    else:\n",
    "        return len(text)\n",
    "\n",
    "# Function where LDA is trained on both the trainf and test. Both datasets are returned along with the model and count_vect\n",
    "def train_lda(df_train, df_test, n_topics, ld, text):\n",
    "\n",
    "    count_vect = CountVectorizer()\n",
    "    bow_counts_train = count_vect.fit_transform(df_train[text].values)\n",
    "    bow_counts_test = count_vect.transform(df_test[text].values)\n",
    "\n",
    "    lda = LatentDirichletAllocation(n_components=n_topics, max_iter=5,\n",
    "                                learning_method='online',\n",
    "                                learning_offset=50.,\n",
    "                                learning_decay=ld)\n",
    "\n",
    "    X_train_lda = lda.fit_transform(bow_counts_train)\n",
    "    X_test_lda = lda.transform(bow_counts_test)\n",
    "\n",
    "    return X_train_lda, X_test_lda, lda, count_vect\n",
    "\n",
    "\n",
    "# Function to print the top words in a topic\n",
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    norm = model.components_.sum(axis=1)[:, np.newaxis]\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(80 * \"-\")\n",
    "        print(\"Topic {}\".format(topic_idx))\n",
    "        for i in topic.argsort()[:-n_top_words - 1:-1]:\n",
    "            print(\"{:.3f}\".format(topic[i] / norm[topic_idx][0]) \n",
    "                  + '\\t' + feature_names[i])\n",
    "\n",
    "# Function to visualize the topics in a wordcloud\n",
    "def visualize_topics(lda, count_vect, terms_count):\n",
    "    terms = count_vect.get_feature_names()\n",
    "    for idx, topic in enumerate(lda.components_):\n",
    "        title = 'Topic ' + str(idx+1)\n",
    "        abs_topic = abs(topic)\n",
    "        topic_terms = [[terms[i],topic[i]] for i in abs_topic.argsort()[:-terms_count-1:-1]]\n",
    "        topic_terms_sorted = [[terms[i], topic[i]] for i in abs_topic.argsort()[:-terms_count - 1:-1]]\n",
    "        topic_words = []\n",
    "        for i in range(terms_count):\n",
    "            topic_words.append(topic_terms_sorted[i][0])\n",
    "            # print(','.join( word for word in topic_words))\n",
    "            # print(\"\")\n",
    "            dict_word_frequency = {}\n",
    "        for i in range(terms_count):\n",
    "            dict_word_frequency[topic_terms_sorted[i][0]] = topic_terms_sorted[i][1]\n",
    "            wcloud = wordcloud.WordCloud(background_color=\"white\",mask=None, max_words=100,\\\n",
    "            max_font_size=60,min_font_size=10,prefer_horizontal=0.9,\n",
    "            contour_width=3,contour_color='black')\n",
    "            wcloud.generate_from_frequencies(dict_word_frequency)\n",
    "        plt.imshow(wcloud, interpolation='bilinear')\n",
    "        plt.axis(\"off\")\n",
    "        plt.title(title, fontsize=20)\n",
    "        # plt.savefig(\"Topic#\"+str(idx+1), format=\"png\")\n",
    "        plt.show()\n",
    "    return\n",
    "\n",
    "#%% RUN LDA\n",
    "\n",
    "# Get data\n",
    "category = 'Candy & Chocolate'\n",
    "train_path = 'data/' + category + '/df_train.csv'\n",
    "test_path = 'data/' + category + '/df_test.csv'\n",
    "df_train = pd.read_csv(train_path)\n",
    "df_test = pd.read_csv(test_path)\n",
    "df_train = df_train.dropna(axis=0,subset=['description'])\n",
    "df_test = df_test.dropna(axis=0,subset=['description'])\n",
    "\n",
    "# Options to tune hyperparamets in LDA model\n",
    "# Beware it will try *all* of the combinations, so it'll take ages\n",
    "search_params = {'n_components': [2, 3, 4], 'learning_decay': [.6, .7, .8]}\n",
    "\n",
    "# Set up LDA with the options we'll keep static\n",
    "model = LatentDirichletAllocation(learning_method='online',\n",
    "                                  max_iter=5,\n",
    "                                  random_state=0)\n",
    "\n",
    "# Try all of the options\n",
    "gridsearch = GridSearchCV(model,\n",
    "                          param_grid=search_params,\n",
    "                          n_jobs=-1,\n",
    "                          verbose=1)\n",
    "count_vect = CountVectorizer()\n",
    "cv_matrix = count_vect.fit_transform(df_train['description'].values)\n",
    "gridsearch.fit(cv_matrix)\n",
    "\n",
    "## Save the best model\n",
    "best_lda = gridsearch.best_estimator_\n",
    "\n",
    "# What did we find?\n",
    "print(\"Best Model's Params: \", gridsearch.best_params_)\n",
    "\n",
    "# Train LDA with best params\n",
    "n_topics = gridsearch.best_params_['n_components']\n",
    "learning_decay = gridsearch.best_params_['learning_decay']\n",
    "\n",
    "# Run LDA on description with tuned parameters\n",
    "X_train_lda, X_test_lda, lda, count_vect = train_lda(df_train, df_test, n_topics, learning_decay, 'description')\n",
    "\n",
    "# Visualize topics as wordclouds\n",
    "visualize_topics(lda, count_vect, 25)\n",
    "\n",
    "# Merge df with lda \n",
    "lda_list = []\n",
    "for i in range(n_topics):\n",
    "    lda_list.append('lda'+str(i+1))\n",
    "X_train_lda_df = pd.DataFrame(X_train_lda, columns = lda_list)\n",
    "X_test_lda_df = pd.DataFrame(X_test_lda, columns = lda_list)\n",
    "df_train_lda = df_train.merge(X_train_lda_df, left_index=True, right_index=True)\n",
    "df_test_lda = df_test.merge(X_test_lda_df, left_index=True, right_index=True)\n",
    "\n",
    "# Save merged data + model\n",
    "today = date.today()\n",
    "df_train_lda.to_csv('data/' + category + '/df_train_lda.csv',index=False)\n",
    "df_test_lda.to_csv('data/' + category + '/df_test_lda.csv',index=False)\n",
    "filename = 'models/'+category+'/lda_model_'+str(today)+'.sav'\n",
    "pickle.dump(lda, open(filename, 'wb'))\n",
    "filename = 'models/'+category+'/count_vect_model_'+str(today)+'.sav'\n",
    "pickle.dump(count_vect, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can conclude, that for our dataset, it is the most effecient, to have a few broad topics. This may also make sence, as our data have already been categorised into smaller datasets being Chocolate & Candy, Beverages and Snack Foods. Maybe if we hadn't already made a subset of the food dataset more topics would be necessary, in order to create the best Topic modeling. The LDA values have been concatenated with the preprocessed dataframe, so every data transformation done so far can be combined and used with a regression model, in order to make good predictions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fd10ce66b02ffab33161025f9ce872921483e68578b0c421a37b8bec3f5cf27b"
  },
  "kernelspec": {
   "display_name": "Python 3.9.1 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
